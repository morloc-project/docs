== The semantic type system 

There is a growing appreciation for the importance of types. Conventionally
dynamic languages, like Python and R, are gaining type systems. 

[source]
----
>>> def foo(x,y):
>>>     print ((x * 2) + y)
>>> foo(2,3)
7
>>> foo("a", "b")
"aab"
>>> foo([1,2,3], [8,9])
[1,2,3,1,2,3,8,9]
>>> def commasep(xs):
>>>     return ', '.join(xs)
>>> commasep(["asdf", "df"])
'asdf, df'
>>> commasep("asdf")
'a, s, d, f'
----

The goal of the semantic type system is to describe the relationship
between general concepts, language-specific constructs, and
language-independent data formats.

The goal of Morloc is to unify all languages under a common type system. This
will allow all pure functions to be ordered into multi-lingual databases. These
functions can then be assembled into pipelines with automatic data reading,
writing, checking, and inter-lingual transer.

The type system is the essential core of Morloc. It is the matrix in which all
other functionallity is nested. It is the interface between Morloc script and
function database. The types are the contract between functions.

Components of a type

. Check at compile time

.. Semantic/ontological types - Orthogonal to the base type, is the semantic
   type system. It is an ontology. I will likely base it on OWL. The semantic
   type system expands upon the base type. For example, the base type might be
   `Double`. But `Double` really is not very informative. The semantic type
   can specialize this into, for example, `Meter`, `Second`, or `RadianAngle`.
   Relations can be set between semantic types (e.g. `is_a Meter Distance`).
   Properties can be added to them, to set the range of the value, add
   a distribution, etc. 

.. Base type - This will consist of a large subset of the Haskell type system.
   Including algebraic types, type classes (will need to include functions
   which are implemented in each language), and parameterized types. The
   purpose of the base type is to allow rigorous type checking and code reuse
   according to property (e.g. all the functor, monoid, foldable, etc wonders
   of Haskell).

. Check at runtime

.. Invariants/runtime dependencies - This includes dependencies between
   sub-units of a type (e.g. arguments in a functional signature or members of
   a tuple). It is probably not practical to have true, compile-time checked,
   dependent types in Morloc, since functions are black boxes. So these
   dependencies will be checked at runtime (unless compiled without checking).

.. Distribution - A random model of the data. All data should be modeled. This
   allows random values to be generated and used in automatic testing. The
   distribution can also be handled in real-time to find outliers and possibly
   pathological behaviour.

. Annotation

.. Performance expectation - There are two layers to this "type" (we are not
   really in type-world anymore, perhaps). First is a performance contract for
   a function signature, where all implementations of the function must meet
   the specified standards. A second layer is a parameterized function for
   modeling time and space usage. This is closer to an annotation. It can be
   inferred through simulation. A major use of this would be to predict
   runtime requirements from the inputs and system data. A programmer may put
   limits on the time and space of the program and halt if a node is predicted
   to exceed them.


=== Function signatures and more stuff

Function signatures are also needed to build the database of functions. I want
a formal way to search for functions that does not depend on human written
documentation or function/argument names. I want to search by type. But it
seems there is currently no good method for this. Within a specific
strongly-typed language, you may be able to search by function signature (e.g.
in Haskell's Hoogle you can search for `[a] -> (a -> b) -> [b]`). Within
dynamic languages you are limited to text searches against the documentation or
unchecked, hand-written type annotations. But when searching for functions
across languages, the best you can do is query Google and hope the terms are
written in the documentation.

A possible way to address these problems is to use a semantic type system. That
is, a type system based on knowledge representation (ontologies). Basing a type
system on formal logic seems reasonable. The ontology of types and the function
signatures are the axioms. The code compiles to a list of statements. The
program is correct if the whole system is logically consistent. Reasoning on
knowledge bases (descriptive logics) is a rich field. There are good languages
(such as OWL), good support for databases (SPARQL), and good reasoners (e.g.
`HermiT` and `Fact++`).

A semantic type system would allow expression of relations between types.
A possible relation might be, "There exists Filename openable as Text". Then if
you pass a Filename to a function expecting Text, the compiler can infer that
Filename is convertible, with possible failure, to Text. It can then search
a database for a suitable function, and if the function is found, insert it
between Filename and Text. By importing different sets of relations, the
strictness of a program can be finely tuned. Behavior that is hard-coded in
most languages, becomes easily customizable. If you want your pipeline to
automatically try to resolve type conflicts by converting a String to a Number
(as is done dynamically in Perl), you just have to add the relation that Number
can be cast as a String (the reasoner can then infer that some String can be
cast as Number).

The type would give the compiler all the information it needs to convert data
between languages. Concrete, language-specific types would map to instances of
the type classes. A given language may have 0 or more instances for a given
type (e.g. the C types `float` and `double` might both be instances of the
Morloc type `Real`). The compiler would sometimes need a hint from the
programmer about which instance to use.

Semantic types offer many additional possibilities. Sets of inter-convertible
types could be specified (e.g. {`Meter`, `Foot`, `Angstrom`} or sequence
alignment formats {`ClustalW`, `Stockholm`}). Types could be given attributes,
such as `has_length` which could be automatically used in descriptive
statistics. The semantic type system could, I think, emulate the typeclasses of
Haskell, e.g. `Monoid` or `Traversable`. Types could be associated with
metadata, such as statistical generative models.

Speaking of statistical generative models, I think most types should have one.
If we can simulate data for all inputs to a function, then we can automatically
test the function, generate sample data, and build simulation-based performance
models. Given performance models, the compiler can reason about the performance
of the overall system and can make smart optimizations.

Overall, I want to give the compiler what it needs so that it can reason about
the code and automate away the tedium of programming.

=== Related work and approach

I originally thought of my semantic type system as a radical departure from
type theory norms. But it is actually less radical than I originally thought
(which is good). The use of semantic reasoners to do type checking is mostly an
implementation difference (using constraint solvers rather than algorithm W)
[@stuckey2006type]. This concept has been explored extensively and has proven
not only feasible but superior to bespoke algorithms (at least in regards to
clarity of debugging info). Allowing types to be terms from a controlled
vocabulary, an ontology, is perhaps still pretty radical. However, it is
probably isomorphic to subtyping or some other system with type inheritance.
Although having an entire rule system describing how the types relate to
eachother is probably more powerful than simple inheritance.

There are three components to the semantic side of Morloc. The 1) knowledge
representation language (KR), 2) the query language (QL), 3) the rule language
(RL) and 4) an inference engine (IE).

KR will be used to store the AST for all scripts, user system info, package
descriptions, community info, and everything else. For KR we can use
RDF/RDFS/OWL. This is a fairly easy choice to make. We will need to decide how
much of OWL to use. There are alternatives, such as using one of the other
graph database systems. Or for that matter, using bespoke data structures.

A QL allows the user to query the entire system. To find functions that fit
a problem. To find workflows where functions of interest were used, for example
to find usage examples. To find subsets of the function space that have certain
standards for strictness, quality, licensing, etc. The QL needs to be able to
query the KR. Given our choice of RDF for KR, SPARQL is a pretty natural
choice. Of course there are a lot of other query languages. But many are
attached to proprietary systems or have low market penetration.

The RL is needed to encode the deep business logic of the system. While many
types of constraints can be encoded in OWL, it has its limitations. Also, OWL
constraints are often not as intuitive as the simple rules that can be written
in, say, Datalog. Here there are many options.

. SPARQL - can be used to impose rules on an ontology.
. SHACL - this is a W3C specification for a SPARQL based constraint language.
. SWRL - the Semantic Web Rule Language allows Datalog-based rules to be used
  to impose laws on an OWL database.  

The IE is needed to infer missing information. This includes

 * type inference (e.g., finding the concrete instances of generic types). 
 * choosing concrete instances for functions when there are multiple choices
   (e.g., which sort function to use).
 * linking functions to their source code
 * etc

These decisions are complex and will require an RL as well. This is also where
many user customizations will occur.

=== Notes

The right way and the fast way. 

 1. semantic type system
    - **Right Way**: Consider category theory (groups, semigroups, algebras),
      search literature, base the ontology on theory. Prove that everything
      works.
    - **Fast Way**: Implement the top-level, less-controversial types. Implement
      deeper ontologies for case studies (e.g. in bioinformatics).
    - Make the type system robust. Equivalent types. Assume graph `is_a`
      relations rather than trees. This allows multiple hierarchies to coexist.

 2. base type
    - **Fast Way**: Follow Haskell conventions. Implement the safe stuff. Use
      the GHC typechecker for handling containers.
    - **Right Way**: Learn from Haskell mistakes. Do Prelude right. Use richer
      algebras. Do not use GHC. Do everything through logical inference. Encode
      containers through the knowledge system. The latter transition from the
      "Fast Way" to the "Right Way" here should be mostly in the backend.

 3. Invariants/runtime dependencies
    - **Fast Way**: Make these annotations, which link to functions that can be
      run in the common language to test data.
    - **Right Way**: Find a formal representation of dependencies. Allow the
      reasoner to work on them.

 4. Distribution
    - **Fast Way**: As with runtime dependencies, add distributions as links to
      generative functions.
    - **Right Way**: Find a formal representation for models. Build a statistical
      reasoning machine to work with them. Test consistency between functions
      across the type system. Allow function instances to specialize the
      models.

 5. Performance expectation
    - **Fast Way**: Add as an informal note in the Annotation
    - **Right Way**: Formally express as a mathematical functions of the
      features derived from the inputs (and the input distributions) and
      possibly the architecture. If the distributions for the inputs are
      specified, then inputs can be sampled and the function timed for each
      input set. The resulting data can be fed into a symbolic regression
      machine to infer an analytic solution to the time and space behaviour.


Making all of this work together is a tricky task. The semantic types can be
used to promote semantic types according to rules, insert conversion functions
(e.g. Meter to Foot), raise errors on unresolvable conflicts, etc. The
dependencies, performance, and distribution types all provide information the
compiler can use to add functions to the graph. Once all this preprocessing and
checking is done, the base type can be checked at compile-time using GHC, along
with a Morloc-side check that typeclass functions are implemented where
expected. 

The type properties (Haskell typeclasses) describe what can be done to a type.
For example: Orderable, Foldable, Traversable, Monoid, Semigroup.

Machines need to learn how to use tools. The statistical deep learning approach
gives them data, and then trains a network to recognize patterns. But whether
this will ever be a substitute for knowledge is uncertain. There are other
approaches to machine learning. Symbolic regression. Logic. Knowledge
representation systems allow them to reason and give them common sense. But how
can they use tools and other resources? How can they find them? They need to be
able to reason about functions. Deep learning can be used to create predicates
in a knowledge representation system.

`[Image] -> PhysicalObject -> [Bool]`
`[(Image, Bool)] -> ([Image] -> PhysicalObject -> [Bool])`

Functions are premises. The reasoner can prove that the program is correct, but
the functions are blackboxes, they may not do what their types suggest. The
functions are premises. If the premises are invalid, then the program is
invalid.

Training data, but for testing. Alternatively, have a generative model.

Advantages of a semantic type system.

OWL2 is based on a formal logic system, Description Logics (DL). It is good for
describing the relations between types and what properties types have. The
types and the type constraints (corresponding to Haskell typeclasses) are
axioms. Morloc code can be translated into a list of facts. If the facts are
consistent, the type checker passes. This will allow highly expressive types.
Strictly supersetting, I believe, the Haskell system (need a proof of this,
does DL superset Hask?).

Individuals are "instances" from the type "classes" (not to be confused with
typeclasses ala Haskell, which are class properties).

OWL as a type system, beyond correctness. Imposing semantic meaning on
functions. This allows them to be organized and searched in databases.

Google is trying to organize all of human knowledge. The semantic web.

The base type can be derived from the semantic type and vice versa. The
semantic type is more specific and contains much information that would never
go into the base type (and that would not be used in type checking).

Ontologies have been springing up all over. Perhaps the largest and most
developed are the biology ones, but there are many more:
[physics](https://www.astro.umd.edu/~eshaya/astro-onto/ontologies/physics.html),
[statistics](http://stato-ontology.org/),
[chemistry](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2867191/).

==== Types by domain

 * statistics
   - John White: [Type Safety and Statistical Computing](http://www.johnmyleswhite.com/notebook/2016/12/12/type-safety-and-statistical-computing/)
     A blog entry on types in statistics. Discusses the need to encode
     assumptions in the type system.

==== Mapping to conventional types

A minimal functionallity of ST is to emulate conventional primitive types and
containers. What information do I need to store about each type?

. Number
.. bounds
. String
.. encoding
.. length
. Boolean
. Enum
. Maybe
.. type

Syntactically, I will probably use a '?' for Maybe, for example: `?Int`

. List
.. length
.. element type
. Tuple
.. discrete number of types
. Matrix
.. dimension
.. element type
. Structure
.. list of (name,type) pairs (recursive)
. Table
.. dimension
.. column types
.. ? column dependencies
.. ? column names
.. ? row names
.. ? table metadata
.. ? column metadata
.. ? row metadata
.. ? cell metadata
. Parameterized Boolean - this is a odd type, but something important I want
  to express: a boolean with semantic annotation. For example:

[source]
----
Filename -> Is Readable
Image    -> Is Dog
String   -> Is Integer
Integer  -> Is Odd
Audio    -> Is HipHop
----

[source]
----
-- general
filterImage :: (Image -> Is Thing) -> [Image] -> [Image]

-- more specific
filterImage :: (Image -> Is Airplane) -> [Image] -> [Image]

-- most specific
filterImage :: (Image -> Is Boeing747) -> [Image] -> [Image]
----

These would have the more general types:

[source]
----
Filename -> Bool
Image    -> Bool
String   -> Bool
Integer  -> Bool
Audio    -> Bool
----

But these lack semantic meaning.

'Is' implies an equivalence of some sort. 'Has' implies a `has_part` relation.
For example: 

[source]
----
[Integer] -> Has 45
Image -> Has Dog
----

. Parameterized Probability

First there is the `ChanceOf` type

[source]
----
FeatureTable -> ChanceOf Win
----
