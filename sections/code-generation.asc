
The input to the code generator is a list of module records.

[source, haskell]
----
   Module
    { moduleName :: MVar                            -- module name
    , modulePath :: Maybe Path                      -- local path to module
    , moduleBody :: [Expr]                          -- full source code
    , moduleExports :: Set EVar                     -- exported morloc variables
    , moduleImports :: [Import]                     -- imported morloc modules
    , moduleImportMap :: Map EVar MVar              -- local term to import map
    , moduleSourceMap :: Map (EVar, Lang) Source    -- foreign language sources
    , moduleTypeMap :: Map EVar TypeSet             -- type of every variable
    , moduleDeclarationMap :: Map EVar Expr         -- all morloc declarations
    }
----

== Root

The first step is to find the modules that are imported by no other module,
i.e., the "root" modules. There should be exactly one root module, which I
shall call "Main".

Each variable that is exported from Main will become a command that can be
called in the generated program.

== Collect

Next a bipartite, ambiguous, abstract syntax tree (AST) is built for each
variabe exported from Main. The AST is implemented as a pair of mutually
recursive data structures.

`SAnno` associates a set of subtrees with a shared annotation (general type and metadata).

[source, haskell]
----
data SAnno g f c = SAnno (f (SExpr g f c, c)) g
----

Where
 * g - an annotation for the group of child trees (what they have in common)
 * f - a collection of subtrees
 * c - an annotation for the specific child tree

`SExpr` stores an expression node and `SAnno` children.

[source, haskell]
----
data SExpr g f c
  = UniS
  | VarS EVar
  | ListS [SAnno g f c]
  | TupleS [SAnno g f c]
  | LamS [EVar] (SAnno g f c)
  | AppS (SAnno g f c) [SAnno g f c]
  | NumS Scientific
  | LogS Bool
  | StrS Text
  | RecS [(EVar, SAnno g f c)]
  | CallS Source
  | ForeignS Int Lang [EVar]
----

== Realize

The `f` in the `SAnno` type takes on two forms in the generator: `One` or `Many`:

[source, haskell]
----
data One a = One a
data Many a = Many [a]
----

The prior "Collect" step gathers all possible programs into one tree, thus `f`
is `Many`. The "Realize" step collapses the tree done to a single "realization"
(or "instance"). Thus the `realize` function, eliding implementation details,
has the type: 

[source, haskell]
----
realize :: SAnno g Many [Type] -> SAnno g One Type
----

Two levels of ambiguity are removed in this step. The `Many` to `One`
transition selects selects a single sub-tree topology. For example, suppose there 

[source]
----
import math (sum, div, length, mean)
source R ("mean")

export mean
mean xs = div (sum xs) (length xs)
----

Here we have three definitions of `mean`. One is sourced from the `R` language
(where it is a built in function). One is sourced from the `morloc math`
module, where it is implemented in C. One is defined as a Morloc composition of
the three functions `div`, `sum`, and `length`; these are all implemented in C
currently, but they could gain more implementations in the future.

There are *three* definitions of `mean`, and *two* topologies (thus two
elements in `Many`). The topologies are either the `(div (sum xs) (length xs))`
tree or the call to the `R` or `C` functions. The first problem is the
`Many->One` selection. The second problem is the `[Type]->Type` problem, where
the sourced implementation is chosen. Here we decide just between a single `R`
and single `C` function. But the choice could be more involved, such as
choosing between a dozen sort algorithms all written in `C`.

This is the data structure is the starting point for an epic optimization
problem.


=== Algorithm optimization

In the distant future, when Morloc is mature, the realization step will
incorporate community knowledge, performance modeling, and benchmarking to make
the optimal decision. For now, I assign a relative cost to each pair of
inter-language calls and find the tree that minimizes the total cost.

The most interesting optimizations those that involve choices between
algorithms. We could build formal performance models for each algorithm and
parameterize them empirically for each implementation.

 * performance
 * accuracy
 * parameterization


=== Build optimization

The goal of build optimization is to 1) ensure the program compiles, 2)
minimize the dependencies and 3) tailor the build to the local architecture. In
theory, a `morloc` program can avoid bit-rot and adapt to any architecture so
long as there exists at least one valid tree instance.

I haven't worked on build optimization yet, but I imagine the input to the
mature morloc build machine will be a description of the local architecture and
a list of possible ASTs, ordered by expected performance. The machine could
then try to build the "best" tree. If the build fails, the machine then finds
the next highest scoring tree that does not contain the failing component.

Making this process efficient through judicious use of deep knowledge gathered
from the community will be a major focus in the future. The knowledge gained in
one build (e.g., function X failed on OS Y in state Z) could be uploaded
automatically to the community knowledgebase and accessed in future builds.


=== Interoperability optimization

The minimization of inter-operability costs is the easiest optimization and the
only one that is currently supported. Program performance can be optimized by
reducing the number and cost of foreign calls.

.Penalties for calls between languages
----
          C    Python           R
C         1       100        1000
Python   10         2        1000
R        10       100           3
----

The values in the table above are obviously very rough, but they demonstrate
important principles for optimizing Morloc programs. Calls within languages are
cheap and between languages are expensive. Major performance improvements could
be obtained by removing the start-up costs of loading the R/Python runtime
(e.g., passing data to an open R server rather than restarting R for every
call).
