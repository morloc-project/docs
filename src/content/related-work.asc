
== Related work

There are several attributes we can use to separate related languages and platforms:

. cached +(C)+ - many pipeline programs, such as the Make family, only build what is needed.
. distributed (D) - are the pieces sent to a cluster or parallelized?
. typed (T) - each node input and output have types, has typechecking 
. language agnostic (A) - any language can be used
. nodes are: [executables (nE), functions (nF)]
.. generative types - Every type is defined as a generative model, when fully
   parameterized, data can be generated from it, for testing purposes. All
   terminating programs are also generative types given the random models of the
   input types.

Based on these discriminants, we can enumerate the types of possible programs:

. C[D]AnE

These tie together many executable scripts. They tend to focus on caching
so that the same steps are not rerun (e.g. Make). This includes most
variants of the Make family.

They may be distributed. Some are simply parallelized (GNU Make) others are
designed to run on Hadoop or on a cluster through Slurm. They emphasize
performance.

Examples: Make, Snakemake

. [CD]TAnE

These are typed, language agnostic pipelines. They coordinate executable
scripts, but annotate them with a type system, such that illegal
combinations are prevented. 

Examples: Galaxy


. [CDT]nF

These are simple pipelines for use within a single language. Whether they
are typed depends on the type system of the host language. They may be
distributed and cached.

Examples: magrittr and rmonad

. [CD]TAnF

Here we have pipeline languages that work at the level of the function, but
that are also language agnostic. That is, any function can be written in
any (supported) language and they can be linked together. The functions do
not have to write their results to STDOUT or read their input from it.

Examples: `morloc`


. [CD]AnF

This may be an impossible language.


=== Related systems

==== Serialization systems

`morloc` is not a serialization system. Indeed, any good serialization system
would be a system that could greatly add to `morloc`, since serialization will
often be the limiting step in a `morloc` workflow's performance. On
serialization system if Google's protobuffers
link:http://reasonablypolymorphic.com/blog/protos-are-wrong/[].

==== IBM Knowledge Studio

link:https://console.bluemix.net/docs/services/knowledge-studio/typesystem.html[IBM Knowledge Studio]
does pure logic reasoning, Doug Lenat style. Also similar to Palantir.

==== Microservices

Microservice oriented design builds systems through modular programs that
interact through usually through a REST interface with intermediate
serialization/deserialization steps.

`morloc` is not a microservice oriented system. The only immediate similarity to
microservices is that calls between functions of different languages will pass
data (barring future optimizations) through a serialization step. The `morloc`
compiler though builds all functions together, integrating them, where
possible, into single programs. Thus functions within languages are communicate
directly, not through an interface.

There is a further, less direct, association with microservices, though. Any
`morloc` program, or composition of programs, can be transformed into
a microservice. This would be a fairly straightforward step, since `morloc`
already knows how to serialize the inputs and output of any function. Thus,
`morloc` will provide microservice deployment of any pipeline.

Any `morloc` pipeline will be able to trivially incorporate a `morloc`
microservice. The company can provide cloud hosting of these microservices. The
code in the microservices could be proprietary. The advantage of hosting on
`morloc` is that, since the code is already on `morloc`, the hosting can be
accomplished with little more than a single click. Then any changes made to the
code that is tested and benchmarked on `morloc`, can be automatically propagated
to the microservice.

link:https://www.oreilly.com/ideas/modules-vs-microservices[modules vs microservices by Sander Mak].

=== Specification languages

[quote, paraphrased from wikipedia]
_____________________________
A specification language is a formal language in computer science that
describes what a process does, rather than how it does it. They should
contain no implementation details. They tend to contain a set of data
values together with a set of functions over those values. The correctness
of the pipeline is dependent on the correctness of the input output
behavior. 
_____________________________

==== petri nets

==== Assertion definition language (ADL)

provides a formal grammar for specifying behaviour using function pre- and
postconditions to specify the interfaces. 

==== Common Algebraic Specification Language

A general-purpose specification language based on first-order logic with induction.

==== Java Modeling Language

Design by contract paradigm

==== Larch family

Has one language of abstract data types and a separate interface language for
each language in which programs are to be written

==== Universal Systems Language

Developed by NASA scientists (Margaret Hamilton) for catching errors before the
occur cite:[hamilton1976higher].

A system of two maps: the function map (FMap) and the type map (TMap). All
systems and their relations are defined in terms of these two maps.

notes paraphrased from link:http://www.htius.com/Articles/r12ham.pdf[]:

Most problems in a programm derive from inconsistencies in the interface.

"Through these efforts, we learned that interface errors (dataflow,
priority, and timing errors from the highest to the lowest levels of
a system to the finest grain) accounted for approximately 75 percent of all
errors -- for example, ambiguous relationships, integration mismatches and
conflicts, communication and coordination problems -- a clear indication
that finding ways to reduce errors in this category was of the highest
priority"

"Although half of the billions of dollars (by today’s standard) spent on the
life cycle was devoted to simulation, 44 percent of the errors were found by
manual means, referred to on the project as the Augekugal method
(eyeballing) or “Nortonizing” (named after the person who perfected this
technique)"

"The interface errors were analyzed in greater detail first because they
not only accounted for the majority of errors, they also were often the
most subtle and most difficult to find. Each interface error was placed
into a category identifying the means to prevent it by way of system
definition. This process led to a set of axioms forming the basis for a new
mathematical theory for designing systems that would, among other things,
eliminate the entire class of interface errors just by the way a system is
defined"

"Implementation- and architecture-independent, USL adheres to the principle
that everything is relative (one person’s design is another’s
implementation); the same language can be used seamlessly throughout
a system’s life cycle to define and integrate all aspects of, and
viewpoints about, the system and its evolution. The overarching principle
is that all aspects of a USL universe are related to the real world and
that the language itself inherently captures this relationship." 

"USL is semantics-dependent but syntax independent"

Recursive reliability


=== Language orientations


==== Semantic-oriented programming

Maxim Kizub, an enigmatic Ukrainian, is the possible coiner of the term
"semantic programming". He is the creator of SymADE, a Java-based IDE. He has
not written any papers that I can find. I also cannot find any reference to
semantic programming in literature. It does, however, show up on Wikipedia.
The original author of the Wikipedia page is, surprise, Maxim Kizub.

So far, the only information I have on him is a few scattered comments and
bug reports. There is more information available in Ukrainian.


==== Intentional programming

The goal of intentional programming is to encode exactly the intent of the
programmer, rather than the operational details.

It was first developed by the Microsoft programmer Charles Simonyi, who
later left Microsoft to found the company Intentional Software. This
company was subsequently reobsorbed by Microsoft.

==== Language-oriented programming

Create a DSL for every problem. Encode specific domain knowledge in
a dedicated language, send it to the user.

MPS - meta programming system (jetbrains).

`morloc` is similar in that I want to encode domain knowledge in the language
itself.

==== Concept programming

There are a lot of neat ideas written in the wikipedia article on Concept
programming. The goals of concept programming and my goals for `morloc` align
well.

See the XL language.

=== Multi-language type systems (schemas) / data serialization systems

==== link:https://microsoft.github.io/bond/why_bond.html[Microsoft Bond]

There is a Bond schema.

==== [Google's Protocol buffers]() - serialization
==== link:http://thrift.apache.org/[Apache Thrift] - link:http://thrift.apache.org/tutorial/[tutorial]
==== [Apache Avro]()

Also, a little different, but link:https://docs.racket-lang.org/quick/index.html[racket]

=== Related work

. nothing - the programming teams choose a language, develop their own
    infrastructure, and write all in one language. They write their own
    wrappers around foreign functions when they need them.

. Platforms based on curated sets of functions - Galaxy, Taverna

In `morloc`, programmers have to agree upon a common type system. In conventional
workflow systems, programmers have to agree upon file specs, error
transmission, log handling, etc. Imagine a two function pipeline: 1) `clean`:
an R script that downloads data from an online source and cleans it into a form
appropriate for downstream analysis; 2) `analyze`: a python script that takes
a table of clean data and performs some analysis on it. These two functions are
written by people who don't know eachother. They are in entirely different
fields and follow entirely different conventions.

The separation between script and component, developed formally in the context
of composition languages, is a core principle of all workflow approaches
(Schneider 1999). The script specifies the connectivity of the components, the
components perform the actual data transforms. 

There are also many languages and programs designed to meet challenges of
writing quality scientific workflows (Leipzig 2016}.

The general idea of extending a program (workflow) without modifying it has
been extensively explored (Hannemann 2002).

In the following sections I will summarize the main classes of
languages/programs that are related to `morloc`.

`morloc` goes further, in that nothing meaningful can be written in `morloc`
without accessing foreign languages. And no language is given special status.

as has mixed language data types (Einarsson 1986).

In the formal composition languages, one programmer writes components and
another programmer writes scripts. Many of the workflow languages are designed
for researchers who can't program. The components are made by a programmer,
wrapped manually in an API, and loaded into a GUI. The composer then uses these
and generally cannot modify them.

.A comparison of features. GWFM refers to Graphical Workflow Managers. `Make*` refers to the family of Make-like rule-based programs.
[%header,format=csv]
,===
`morloc`, GWFM, Shell, Make*, Cuneiform

Component         , function     , wrapper    , executable      , recipe      , function
Script            , composition  , node graph , pipeline/ad hoc , build graph , composition
Component IO      , builtin      , system     , system          , system      , system
Foreign call IO   , native       , wrapper    , wrapper         , wrapper     , native
Function scope    , composition  , component  , component       , local       , component
Type System       , yes          , yes        , none            , none        , filetype
Remote access     , yes          , ?          , no              , no          , no
Remote validation , yes          , ?          , no              , no          , no
Control syntax    , user/library , builtin*   , builtin         , none        , builtin
,===

. Component: the form of a node in the workflow.
. recipes: used in the Make languages are a list of instructions. Usually
  these are written in a shell language usually, but Drake supports several
  additional languages.
. Script: the form of the workflow.
. Component IO: how is input passed to and from a function:
.. idiomatic: implies it is passed as idiomatic data structures in the source language 
.. system: implies it is read from files or databases (components cannot
    pass native data structures).
. Foreign call IO:
. Function scope: xxx.
. **Remote access** and **validation**: Can the language access node values
  or add validators without local modification of the code?.
. **Control syntax**: Origin of control structures (e.g. conditionals,
  loops). In `morloc` these are in user space (or core libraries). GWFM vary,
  many have no conditionals or loops. Make derivatives have conditionals but
  no loops.

==== Formal compositional languages

A powerful abstraction is the separation of a program into components and
a script (schneider 1999). In the computer science community, this is the
central concept of component languages, such as Piccola (Achermann 2001).
Outside this community community, the concept is widespread (though unsung) in
workflow programs.

Piccola is built on a model that cleanly separates the script from the
components (Achermann 2001). The components in Piccola are considered black
boxes. They argue that OOP is good for building components but bad at reusing
them, because the components become overly specialized and the interfaces
overly complex, such that scripting with them requires deep understanding of
their implementation. `morloc` follows the same model of scripts and components.
The components in Picolla are general objects, characterized by an API. The
components can be anything. In `morloc` the components are functions. They can be
used in exactly one way, which is specified by their type signature.

The work most similar to `morloc` is perhaps the language Cuneiform (brandt
2015), which is a workflow language designed for high-performance and implicit
parallelism. Cuneiform allows multiple languages and is uses explicit types.
`morloc` differs in its emphasis on automating IO code and type conversions
between languages.

==== Scripting languages

Many scientific workflows are coordinated through scripting languages, usually
Bash (or a similar shell language). While Bash can be used as a full
programming language, it is more often used to link components (programs)
written in other languages. The input and output of these functions must be in
a common representation (raw data in files or connections to data bases). Thus
the components handle the IO.

There are also specialized scientific scripting languages, for example,
BigDataScript (Cingolani 2015}, which is optimized for job submission and
management within a cluster environment.


==== Rule-based languages

The most well-known rule-based language is Make. Its was designed for
coordinating the build of a program. It consists of a set of rules for
producing new files and managing file dependencies. Make is often used in
scientific analysis (e.g. (Askren 2016)). A few variants of Make designed for
scientific analysis include Snakemake (Koster 2012)} and Drake
(github.com/Factual/drake). The dependency graphs automatically built by Make
are isomorphic to the pull-based workflows of `morloc`. But Make (and its
variants) provide no type safety or language integration. Drake does allow
recipes to be written in many languages, but by design data can passed between
recipes only through the files they create.


==== Graphical workflow managers

Galaxy (Hillman 2012), Taverna (Oinn 2004) and other
workflow managers allow visual composition of functional units and setting of
parameters. They are designed for the non-programmer, thus the community that
composes workflows and the community that creates the nodes, are quite
separate.

Thus they are an interesting case of the component model where the composers
and scripters are members of different communities.  Those who use the
components are not generally the ones who write them (or could write them).

In contrast, `morloc` is a metaprogramming language for creating pure code.

=== Summary of related work

The different approaches to workflows can be classified based on the nature of
the script, component, coder (writer of components), and composer (writer of
scripts). In the composition languages, components are objects. In scripting
languages, components are generally executable programs.  In graphical workflow
managers components are hard-coded functions wrapped in a graphical widget with
exposed parameters, inputs and outputs. The differences between approaches is
summarized in Table xxx

They also differ in how data how they grow in complexity, handle IO, and their
type system.

=== Workflow theory (and other)

TODO: reread the charfi2006aspect paper, look into who cited it

TODO: factcheck, 'there are no aspect oriented workflow languages'

The idea of adding aspect-oriented elements to a workflow languages has been
explored in the context of business workflows (Charfi 2006) but has not been
integrated into a programming language.

General workflow languages, for example YAWL (Van 2005). YAWL is based on petri
nets (Murata 1989), which are a mathematical system for describing networks.

. Taverna (Oinn 2004), a graphical program for linking web services. It also
  supports R scripts (which must be wrapped as Rshells) and running
  standalone programs.
. OMICtools, a database of bioinformatics functions
. rperl - collections of functions link:http://bioinformaticssoftwareandtools.co.in/rperl.php[]
. Galaxy, similar to Taverna
. Pegasus takes an XML abstract workflow description as input and manages the
  execution environment.
. BioWMS (Bartocci 2007)
. Biowep (Romano 2007)
. Bioworks (Han 2011)
. Conveyor (Linke 2011)
. Mobyle (Neron 2009)
. Ergatis (Orvis 2010)

Node-based graphical programs: Galaxy, node programming editors in (e.g.  in
Blender).

Rule based bioinformatics workflows (Conery 2005).

Modeling bioinformatics workflows with petri-net models (Peleg 2002).

Outside of bioinformatics, there is the Kepler system (Ludascher 2006).

There are also many specialized workflows. Generally these have a small number
of 'hard-coded' components.

A few more things to look at: Triana, SOMA, SMILA, VisTrails, Discovery Net,
Pipeline Pilot, KNIME
