// The goal of the semantic type system is to describe the relationship
// between general concepts, language-specific constructs, and
// language-independent data formats.
//
// The goal of `morloc` is to unify all languages under a common type system. This
// will allow all pure functions to be ordered into multi-lingual databases. These
// functions can then be assembled into pipelines with automatic data reading,
// writing, checking, and inter-lingual transer.
//
// The type system is the essential core of `morloc`. It is the matrix in which all
// other functionallity is nested. It is the interface between `morloc` script and
// function database. The types are the contract between functions.

== The semantic type system 

The fundamental innovation of `morloc` is the merger of knowledge engineering,
type system design, and functional programming to create a powerful new
programming paradigm. The foundation of this new paradigm is the concept of a
universal semantic type system, where all information about the data and
functions are stored using ontologies as knowledge representation systems.
`morloc` uses ontologies for three distinct purposes: 1) as a language-agnostic
type system, 2) to power automatic programming through encoded domain knowledge
and 3) to describe data, functions, and systems. These purposes will be
detailed in the following paragraphs. 

*First*, the ontologies are the foundation of the universal type system. Like
other strong type systems, the `morloc` type system will prevent many bugs at
compile time. It goes beyond most languages by also allowing specification of
constraints on values (e.g. input x must be greater than input y). These
constraints can be used in static analysis of program correctness or to
generate runtime assertions. They effectively replace in-code assertion
statements and informal English descriptions of behavior with concise,
language-agnostic mathematical statements about the function. Since the type
system is language-agnostic, `morloc` provides a common language for programmers
and domain scientists to discuss the operation and assumptions of functions.
This system is a necessary requirement for the vision of building a universal
library.

*Second*, the type system can be augmented with domain knowledge to provide
rich machine and human accessible documentation, powerful and highly
customizable automatic programming, and more intelligent data validation. The
domain knowledge is encoded as relations between types or as annotations of
types with special information. Examples of the knowledge that may be encoded
include: inheritance relationships between types (e.g., `WindSpeed` inherits
from `Speed`); associations between functions and the functions that benchmark
or predict their performance; associations between data types and random
generators; identification of conversion functions (see Fig. 6). 

*Third*, the ontologies can store metadata about functions, data and systems.
This would include metadata about functions (such as the version, license,
documentation, author, performance metrics, links to related functions,
keywords) or about data (such as its historical origin) or about systems (such
as details about how languages work together, which compilers to use, or the
local user preferences and system architecture). Examples of the application of
this knowledge include: using community experience to optimize builds; using
license data to avoid violations in proprietary workflows; and using author
information to automatically cite the creators of the package or submit error
reports.

These three purposes intersect to offer human and machine users a powerful
resource discovery system. There is currently no general way to search for
tools and functions across different languages, because within programming
languages the meaning of the code is hidden in variable naming conventions and
documentation that is inaccessible to machines. By unifying programming
languages and tools under a common type system, search engines can be designed
to find and compare functions by their roles and input/output types. The common
type system will also allow machines, and emergent AIs, to reason about
`morloc`â€™s tools. 

By lifting this entire system into a community portal, an environment is
created that allows both humans and machines to participate in curating and
building a collective description of the world (the data type system), a set of
tools to operate on it (the function library), and knowledge about how to use
the tools (the function type system). The global ontologies further offer
endless potential for `morloc` extensions that improve pipeline documentation,
automation, optimization, accessibility, and reproducibility.

// === Function signatures and more stuff
//
// Function signatures are also needed to build the database of functions. I want
// a formal way to search for functions that does not depend on human written
// documentation or function/argument names. I want to search by type. But it
// seems there is currently no good method for this. Within a specific
// strongly-typed language, you may be able to search by function signature (e.g.
// in Haskell's Hoogle you can search for `[a] -> (a -> b) -> [b]`). Within
// dynamic languages you are limited to text searches against the documentation or
// unchecked, hand-written type annotations. But when searching for functions
// across languages, the best you can do is query Google and hope the terms are
// written in the documentation.
//
// A possible way to address these problems is to use a semantic type system. That
// is, a type system based on knowledge representation (ontologies). Basing a type
// system on formal logic seems reasonable. The ontology of types and the function
// signatures are the axioms. The code compiles to a list of statements. The
// program is correct if the whole system is logically consistent. Reasoning on
// knowledge bases (descriptive logics) is a rich field. There are good languages
// (such as OWL), good support for databases (SPARQL), and good reasoners (e.g.
// `HermiT` and `Fact++`).
//
// A semantic type system would allow expression of relations between types.
// A possible relation might be, "There exists Filename openable as Text". Then if
// you pass a Filename to a function expecting Text, the compiler can infer that
// Filename is convertible, with possible failure, to Text. It can then search
// a database for a suitable function, and if the function is found, insert it
// between Filename and Text. By importing different sets of relations, the
// strictness of a program can be finely tuned. Behavior that is hard-coded in
// most languages, becomes easily customizable. If you want your pipeline to
// automatically try to resolve type conflicts by converting a String to a Number
// (as is done dynamically in Perl), you just have to add the relation that Number
// can be cast as a String (the reasoner can then infer that some String can be
// cast as Number).
//
// The type would give the compiler all the information it needs to convert data
// between languages. Concrete, language-specific types would map to instances of
// the type classes. A given language may have 0 or more instances for a given
// type (e.g. the C types `float` and `double` might both be instances of the
// `morloc` type `Real`). The compiler would sometimes need a hint from the
// programmer about which instance to use.
//
// Semantic types offer many additional possibilities. Sets of inter-convertible
// types could be specified (e.g. {`Meter`, `Foot`, `Angstrom`} or sequence
// alignment formats {`ClustalW`, `Stockholm`}). Types could be given attributes,
// such as `has_length` which could be automatically used in descriptive
// statistics. The semantic type system could, I think, emulate the typeclasses of
// Haskell, e.g. `Monoid` or `Traversable`. Types could be associated with
// metadata, such as statistical generative models.
//
// Speaking of statistical generative models, I think most types should have one.
// If we can simulate data for all inputs to a function, then we can automatically
// test the function, generate sample data, and build simulation-based performance
// models. Given performance models, the compiler can reason about the performance
// of the overall system and can make smart optimizations.
//
// Overall, I want to give the compiler what it needs so that it can reason about
// the code and automate away the tedium of programming.


// === Components of the semantic system
//
// The use of semantic reasoners for type processing is mostly an implementation
// difference (using constraint solvers rather than algorithm W)
// cite:[stuckey2006type]. This concept has been explored extensively and has
// proven not only feasible but superior to bespoke algorithms (at least in
// regards to clarity of debugging info) cite:[stuckey2006type]. Allowing types to
// be terms from a controlled vocabulary, an ontology, is major extension to the
// system.
//
// There are four components to the semantic side of `morloc`. The 1) knowledge
// representation language (KR), 2) the query language (QL), 3) the rule language
// (RL) and 4) an inference engine (IE).
//
// ==== Knowledge representation language
//
// KR will be used to store the abstract syntax tree for all scripts, user system
// info, package descriptions, community info, and everything else. For KR we can
// use RDF/RDFS/OWL. We will need to decide how expressive a system we need (e.g.
// a subset of OWL2, all of OWL2, higher order logics, etc). There are
// alternatives, such as using one of the other graph database systems. Or for
// that matter, using bespoke data structures.
//
// OWL2 is based on a formal logic system, Description Logics (DL). It is good for
// describing the relations between types and what properties types have. The
// types and the type constraints (corresponding to Haskell typeclasses) are
// rules. `morloc` code can be translated into a list of facts. If the facts are
// consistent, the type checker passes.
//
// All data are "instances" from the type "classes" (not to be confused with
// typeclasses ala Haskell, which are class properties). Functions are premises.
// The reasoner can prove that the program is correct, but the functions are
// blackboxes (for now), they may not do what their types suggest. The functions
// are premises. If the premises are invalid, then the program is invalid.
//
// ==== Query language
//
// A QL allows the user to query the entire system to 1) find functions that fit a
// problem, 2) find workflows where functions of interest were used (usage
// example), 3) find subsets of the function space that have certain standards for
// strictness, quality, licensing, etc. The QL needs to be able to query the KR.
// Given our choice of RDF for KR, SPARQL is a pretty natural choice. Of course
// there are a lot of other query languages, but many are attached to proprietary
// systems or have low market penetration.
//
// A `morloc` script itself can be seen as a kind of query against the universal
// library. Each function in the script may have many implementations. The
// `morloc` compiler must select which implementations to use in the final
// generated executable. The query nature of `morloc` script could be made more
// explicit by adding concrete constraints (e.g., `?f ?x = 4 when ?x = 16`, where
// `?f` is an unknown function). The query would return many possible programs,
// sorted by predicted performance, if one fails to build, then the problem can be
// automatically diagnosed and an alternative program can be built. Such
// functionallity is still far in the future, however.
//
// ==== Rule language
//
// The RL is needed to encode the deep business logic of the system. While many
// types of constraints can be encoded in OWL, it has its limitations. Also, OWL
// constraints are often not as intuitive as the simple rules that can be written
// in, say, Datalog. There are many options.
//
// . SPARQL - can be used to impose rules on an ontology.
// . SHACL - this is a W3C specification for a SPARQL based constraint language.
// . SWRL - the Semantic Web Rule Language allows Datalog-based rules to be used
//   to impose laws on an OWL database.
//
// ==== Inference engine
//
// The IE is needed to infer missing information. This includes
//
//  * type inference (e.g., finding the concrete instances of generic types).
//  * choosing concrete instances for functions when there are multiple choices
//    (e.g., which sort function to use).
//  * linking functions to their source code
//
// These decisions are complex and will require an RL as well. This is also where
// many user customizations will occur.

// === How `morloc` uses knowledge
//
// ==== Static type checking
//
// .Base type
// The base type will consist of a large subset of the Haskell type system.
// Including algebraic types, type classes (will need to include functions which
// are implemented in each language), and parameterized types. The purpose of the
// base type is to allow rigorous type checking and code reuse according to
// property (e.g. functor, monoid, foldable, and other wonders of Haskell).
//
// .Semantic/ontological types
// Orthogonal to the base type, is the semantic type system. The semantic type
// system expands upon the base type. For example, the base type might be
// `Double`. But `Double` really is not very informative. The semantic type can
// specialize this into, for example, `Meter`, `Second`, or `RadianAngle`.
// Relations can be set between semantic types (e.g. `is_a Meter Distance`).
// Properties can be added to them, to set the range of the value, add a
// distribution, etc.
//
// Before settling on the semantic type system, we should carefully consider
// category theory (groups, semigroups, algebras), search literature, base the
// ontology on theory. We should prove that everything works, if possible.

// ==== Runtime data validation
//
// .Invariants/runtime dependencies
// These includes dependencies between sub-units of a type (e.g. arguments in a
// functional signature or members of a tuple). It is probably not practical to
// have true, compile-time checked, dependent types in `morloc`, since functions are
// black boxes. So these dependencies will be probably be checked at runtime
// (unless compiled without checking). In the future, at least in some cases, we
// will be able to statically analyze the invariants.
//
// .Distribution
// A random model of the data. All data should be modeled. This allows random
// values to be generated and used in automatic testing. The distribution can also
// be handled in real-time to find outliers and possibly pathological behaviour.
// In the short term, we can add distributions as links to generative functions.
// In the long term, we should find a formal representation for models. Build a
// statistical reasoning machine to work with them. Test consistency between
// functions across the type system. Allow function instances to specialize the
// models.
//
// . Annotation
// Performance expectation - There are two layers to this "type" (we are not
// really in type-world anymore, perhaps). First is a performance contract for a
// function signature, where all implementations of the function must meet the
// specified standards. A second layer is a parameterized function for modeling
// time and space usage. This is closer to an annotation. It can be inferred
// through simulation. A major use of this would be to predict runtime
// requirements from the inputs and system data. A programmer may put limits on
// the time and space of the program and halt if a node is predicted to exceed
// them.
//
// Eventually we should formally express as a mathematical functions of the
// features derived from the inputs (and the input distributions) and possibly the
// architecture. If the distributions for the inputs are specified, then inputs
// can be sampled and the function timed for each input set. The resulting data
// can be fed into a symbolic regression machine to infer an analytic solution to
// the time and space behaviour.
//
// ==== Allowing machines to use our tools
//
// Machines need to learn how to use tools. The statistical deep learning approach
// gives them data, and then trains a network to recognize patterns. But whether
// this will ever be a substitute for knowledge is uncertain. There are other
// approaches to machine learning: Symbolic regression and logic. Knowledge
// representation systems allow them to reason and give them common sense. But how
// can they use tools and other resources? How can they find them? They need to be
// able to reason about functions. Deep learning can be used to create predicates
// in a knowledge representation system.

// ==== Searching for functions
//
// Imposing semantic meaning on functions. This allows them to be organized and
// searched in databases.
//
// ==== Types by domain
//
// .Statistics
//
//   . John White: The blog post [Type Safety and Statistical Computing](http://www.johnmyleswhite.com/notebook/2016/12/12/type-safety-and-statistical-computing/)
//   discusses the need to encode assumptions in the type system


// ==== Mapping to conventional types
//
// A minimal functionallity of ST is to emulate conventional primitive types and
// containers. What information do I need to store about each type?
//
// . Number
// .. bounds
// . String
// .. encoding
// .. length
// . Boolean
// . Enum
//
// . List
// .. length
// .. element type
// . Tuple
// . Matrix
// .. dimension
// .. element type
// . Structure
// .. list of (name,type) pairs (recursive)
// . Table
// .. dimension
// .. column types
// .. ? column dependencies
// .. ? column names
// .. ? row names
// .. ? table metadata
// .. ? column metadata
// .. ? row metadata
// .. ? cell metadata
// . Parameterized Boolean - this is a odd type, but something important I want
//   to express: a boolean with semantic annotation. For example:

// [source]
// ----
// Filename -> Is Readable
// Image    -> Is Dog
// String   -> Is Integer
// Integer  -> Is Odd
// Audio    -> Is HipHop
// ----
//
// [source]
// ----
// -- general
// filterImage :: (Image -> Is Thing) -> [Image] -> [Image]
//
// -- more specific
// filterImage :: (Image -> Is Airplane) -> [Image] -> [Image]
//
// -- most specific
// filterImage :: (Image -> Is Boeing747) -> [Image] -> [Image]
// ----
//
// These would have the more general types:
//
// [source]
// ----
// Filename -> Bool
// Image    -> Bool
// String   -> Bool
// Integer  -> Bool
// Audio    -> Bool
// ----
//
// But these lack semantic meaning.
//
// 'Is' implies an equivalence of some sort. 'Has' implies a `has_part` relation.
// For example:
//
// [source]
// ----
// [Integer] -> Has 45
// Image -> Has Dog
// ----
//
// . Parameterized Probability
//
// First there is the `ChanceOf` type
//
// [source]
// ----
// FeatureTable -> ChanceOf Win
// ----


// === Why do we need type systems?
//
// There is a growing appreciation for the importance of types. Conventionally
// dynamic languages, like Python and R, are gaining type systems.
//
// [source]
// ----
// >>> def foo(x,y):
// >>>     print ((x * 2) + y)
// >>> foo(2,3)
// 7
// >>> foo("a", "b")
// "aab"
// >>> foo([1,2,3], [8,9])
// [1,2,3,1,2,3,8,9]
// >>> def commasep(xs):
// >>>     return ', '.join(xs)
// >>> commasep(["asdf", "df"])
// 'asdf, df'
// >>> commasep("asdf")
// 'a, s, d, f'
// ----
